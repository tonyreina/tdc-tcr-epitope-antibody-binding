{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bioinformatics model for protein therapeutics\n",
    "\n",
    "We'll use the [Therapeutics Data Commons](https://tdcommons.ai/) Python package to download open-source ([CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)) datasets that are meaningful in pharmaceutical research. In this notebook, we'll use a dataset called [TCR-Epitope Binding Affinity](https://tdcommons.ai/multi_pred_tasks/tcrepitope/).\n",
    "\n",
    "![TCR-epitope binding](tcr-epitope-binding.png)\n",
    "\n",
    "We show how to create a deep learning model for predicting if a T-cell receptor (TCR) and protein epitope will bind to each other. A model that can predict how well a TCR bindings to an epitope can lead to more effective treatments that use immunotherapy. For example, in anti-cancer therapies it is important for the T-cell receptor to bind to the protein marker in the cancer cell so that the T-cell (actually the T-cell's friends in the immune system) can kill the cancer cell.\n",
    "\n",
    "We'll see how to use the open-sourced [bio-embeddings](https://docs.bioembeddings.com/v0.2.3/) Python library to get the latest state-of-the-art AI model for embedding the protein sequences. In this case, we use Facebook's open-source [Evolutionary Scale Model (ESM-1b)](https://github.com/facebookresearch/esm). These embeddings turn the protein sequences into a vector of 1,280 numbers that the computer can use in a mathematical model. The vector of numbers uniquely encodes (aka embeds) a protein sequence in the same way that the [Dewey Decimal System](https://en.wikipedia.org/wiki/Dewey_Decimal_Classification) uniquely encodes a book into a set of numbers (and letters).\n",
    "\n",
    "Then, we'll show how to combine this embedding with a simple neural network to create a [binary classifier](https://en.wikipedia.org/wiki/Binary_classification) for the TCR-epitope binding affinity prediction (True=They Bind, False=They don't bind).\n",
    "\n",
    "![encoder-decoder Dewey Decimal](encoder-decoder.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas(\n",
    "    desc=\"Embedding protein sequences\"\n",
    ")  # Create fancy progress bar for Pandas apply"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the dataset\n",
    "\n",
    "We are using the TDC dataset for [TCR-Epitope Binding Affinity Prediction Task](https://tdcommons.ai/multi_pred_tasks/tcrepitope/).\n",
    "\n",
    "From the TDC website:\n",
    "\n",
    ">T-cells are an integral part of the adaptive immune system, whose survival, proliferation, activation and function are all governed by the interaction of their T-cell receptor (TCR) with immunogenic peptides (epitopes). A large repertoire of T-cell receptors with different specificity is needed to provide protection against a wide range of pathogens. This new task aims to predict the binding affinity given a pair of TCR sequence and epitope sequence.\n",
    "\n",
    ">Weber et al.\n",
    "Dataset Description: The dataset is from Weber et al. who assemble a large and diverse data from the VDJ database and ImmuneCODE project. It uses human TCR-beta chain sequences. Since this dataset is highly imbalanced, the authors exclude epitopes with less than 15 associated TCR sequences and downsample to a limit of 400 TCRs per epitope. The dataset contains amino acid sequences either for the entire TCR or only for the hypervariable CDR3 loop. Epitopes are available as amino acid sequences. Since Weber et al. proposed to represent the peptides as SMILES strings (which reformulates the problem to protein-ligand binding prediction) the SMILES strings of the epitopes are also included. 50% negative samples were generated by shuffling the pairs, i.e. associating TCR sequences with epitopes they have not been shown to bind.\n",
    "\n",
    ">Task Description: Binary classification. Given the epitope (a peptide, either represented as amino acid sequence or as SMILES) and a T-cell receptor (amino acid sequence, either of the full protein complex or only of the hypervariable CDR3 loop), predict whether the epitope binds to the TCR.\n",
    "\n",
    ">Dataset Statistics: 47,182 TCR-Epitope pairs between 192 epitopes and 23,139 TCRs.\n",
    "\n",
    ">References:\n",
    "\n",
    "1. Weber, Anna, Jannis Born, and María Rodriguez Martínez. “TITAN: T-cell receptor specificity prediction with bimodal attention networks.” Bioinformatics 37.Supplement_1 (2021): i237-i244.\n",
    "\n",
    "2. Bagaev, Dmitry V., et al. “VDJdb in 2019: database extension, new analysis infrastructure and a T-cell receptor motif compendium.” Nucleic Acids Research 48.D1 (2020): D1057-D1062.\n",
    "\n",
    "3. Dines, Jennifer N., et al. “The immunerace study: A prospective multicohort study of immune response action to covid-19 events with the immunecode™ open access database.” medRxiv (2020).\n",
    "\n",
    ">Dataset License: CC BY 4.0.\n",
    "\n",
    ">Contributed by: Anna Weber and Jannis Born.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the TCR-epitope dataset\n",
    "\n",
    "Download and split randomly into 70% training data, 10% validation data, and 20% testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdc.multi_pred import TCREpitopeBinding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "100%|██████████| 16.0M/16.0M [00:00<00:00, 41.0MiB/s]\n",
      "Loading...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "data = TCREpitopeBinding(name=\"weber\", path=\"./data\")  # Download the dataset\n",
    "split = data.get_split(\n",
    "    method=\"random\", seed=816, frac=[0.7, 0.1, 0.2]\n",
    ")  # Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: \t\t33,028 proteins\n",
      "Validation dataset size: \t 4,718 proteins\n",
      "Test dataset size: \t\t 9,436 proteins\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train dataset size: \\t\\t{len(split['train']):6,d} proteins\")\n",
    "print(f\"Validation dataset size: \\t{len(split['valid']):6,d} proteins\")\n",
    "print(f\"Test dataset size: \\t\\t{len(split['test']):6,d} proteins\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epitope_aa</th>\n",
       "      <th>epitope_smi</th>\n",
       "      <th>tcr</th>\n",
       "      <th>tcr_full</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FLKEKGGL</td>\n",
       "      <td>CC(C)C[C@H](NC(=O)CNC(=O)CNC(=O)[C@H](CCCCN)NC...</td>\n",
       "      <td>CSVWGTGKTYEQYF</td>\n",
       "      <td>SAVISQKPSRDICQRGTSLTIQCQVDSQVTMMFWYRQQPGQSLTLI...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FLKEKGGL</td>\n",
       "      <td>CC(C)C[C@H](NC(=O)CNC(=O)CNC(=O)[C@H](CCCCN)NC...</td>\n",
       "      <td>CSVWGEGRSYEQYF</td>\n",
       "      <td>SAVISQKPSRDICQRGTSLTIQCQVDSQVTMMFWYRQQPGQSLTLI...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FLKEKGGL</td>\n",
       "      <td>CC(C)C[C@H](NC(=O)CNC(=O)CNC(=O)[C@H](CCCCN)NC...</td>\n",
       "      <td>CSATILAGVPYGEQYF</td>\n",
       "      <td>GAVVSQHPSWVICKSGTSVKIECRSLDFQATTMFWYRQFPKQSLML...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FLKEKGGL</td>\n",
       "      <td>CC(C)C[C@H](NC(=O)CNC(=O)CNC(=O)[C@H](CCCCN)NC...</td>\n",
       "      <td>CASSFDREVTGELFF</td>\n",
       "      <td>GAGVSQTPSNKVTEKGKYVELRCDPISGHTALYWYRQSLGQGPEFL...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FLKEKGGL</td>\n",
       "      <td>CC(C)C[C@H](NC(=O)CNC(=O)CNC(=O)[C@H](CCCCN)NC...</td>\n",
       "      <td>CASSVGAGTEAFF</td>\n",
       "      <td>DGGITQSPKYLFRKEGQNVTLSCEQNLNHDAMYWYRQDPGQGLRLI...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33023</th>\n",
       "      <td>KLMNIQQKL</td>\n",
       "      <td>CC[C@H](C)[C@H](NC(=O)[C@H](CC(N)=O)NC(=O)[C@H...</td>\n",
       "      <td>CASSKPGLTDTQYF</td>\n",
       "      <td>NAGVTQTPKFQVLKTGQSMTLQCAQDMNHEYMSWYRQDPGMGLRLI...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33024</th>\n",
       "      <td>TLIGDCATV</td>\n",
       "      <td>CC[C@H](C)[C@H](NC(=O)[C@H](CC(C)C)NC(=O)[C@@H...</td>\n",
       "      <td>CASSPGQGRTHYGYTF</td>\n",
       "      <td>NAGVTQTPKFRILKIGQSMTLQCAQDMNHNYMYWYRQDPGMGLKLI...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33025</th>\n",
       "      <td>LLFGYPVYV</td>\n",
       "      <td>CC(C)C[C@H](N)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H]...</td>\n",
       "      <td>CASSGGSLNTEAFF</td>\n",
       "      <td>NAGVTQTPKFQVLKTGQSMTLQCAQDMNHEYMSWYRQDPGMGLRLI...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33026</th>\n",
       "      <td>ISPRTLNAW</td>\n",
       "      <td>CC[C@H](C)[C@H](N)C(=O)N[C@@H](CO)C(=O)N1CCC[C...</td>\n",
       "      <td>CASSPSAAMNTEAFF</td>\n",
       "      <td>TVSWYQQALGQGPQFIFQYYREEENGRGNSPPRFSGLQFPNYSSEL...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33027</th>\n",
       "      <td>ILHCANFNV</td>\n",
       "      <td>CC[C@H](C)[C@H](N)C(=O)N[C@@H](CC(C)C)C(=O)N[C...</td>\n",
       "      <td>CASWGSPSSNTQYF</td>\n",
       "      <td>TVSWYQQALGQGPQFIFQYYREEENGRGNSPPRFSGLQFPNYSSEL...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33028 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      epitope_aa                                        epitope_smi  \\\n",
       "0       FLKEKGGL  CC(C)C[C@H](NC(=O)CNC(=O)CNC(=O)[C@H](CCCCN)NC...   \n",
       "1       FLKEKGGL  CC(C)C[C@H](NC(=O)CNC(=O)CNC(=O)[C@H](CCCCN)NC...   \n",
       "2       FLKEKGGL  CC(C)C[C@H](NC(=O)CNC(=O)CNC(=O)[C@H](CCCCN)NC...   \n",
       "3       FLKEKGGL  CC(C)C[C@H](NC(=O)CNC(=O)CNC(=O)[C@H](CCCCN)NC...   \n",
       "4       FLKEKGGL  CC(C)C[C@H](NC(=O)CNC(=O)CNC(=O)[C@H](CCCCN)NC...   \n",
       "...          ...                                                ...   \n",
       "33023  KLMNIQQKL  CC[C@H](C)[C@H](NC(=O)[C@H](CC(N)=O)NC(=O)[C@H...   \n",
       "33024  TLIGDCATV  CC[C@H](C)[C@H](NC(=O)[C@H](CC(C)C)NC(=O)[C@@H...   \n",
       "33025  LLFGYPVYV  CC(C)C[C@H](N)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H]...   \n",
       "33026  ISPRTLNAW  CC[C@H](C)[C@H](N)C(=O)N[C@@H](CO)C(=O)N1CCC[C...   \n",
       "33027  ILHCANFNV  CC[C@H](C)[C@H](N)C(=O)N[C@@H](CC(C)C)C(=O)N[C...   \n",
       "\n",
       "                    tcr                                           tcr_full  \\\n",
       "0        CSVWGTGKTYEQYF  SAVISQKPSRDICQRGTSLTIQCQVDSQVTMMFWYRQQPGQSLTLI...   \n",
       "1        CSVWGEGRSYEQYF  SAVISQKPSRDICQRGTSLTIQCQVDSQVTMMFWYRQQPGQSLTLI...   \n",
       "2      CSATILAGVPYGEQYF  GAVVSQHPSWVICKSGTSVKIECRSLDFQATTMFWYRQFPKQSLML...   \n",
       "3       CASSFDREVTGELFF  GAGVSQTPSNKVTEKGKYVELRCDPISGHTALYWYRQSLGQGPEFL...   \n",
       "4         CASSVGAGTEAFF  DGGITQSPKYLFRKEGQNVTLSCEQNLNHDAMYWYRQDPGQGLRLI...   \n",
       "...                 ...                                                ...   \n",
       "33023    CASSKPGLTDTQYF  NAGVTQTPKFQVLKTGQSMTLQCAQDMNHEYMSWYRQDPGMGLRLI...   \n",
       "33024  CASSPGQGRTHYGYTF  NAGVTQTPKFRILKIGQSMTLQCAQDMNHNYMYWYRQDPGMGLKLI...   \n",
       "33025    CASSGGSLNTEAFF  NAGVTQTPKFQVLKTGQSMTLQCAQDMNHEYMSWYRQDPGMGLRLI...   \n",
       "33026   CASSPSAAMNTEAFF  TVSWYQQALGQGPQFIFQYYREEENGRGNSPPRFSGLQFPNYSSEL...   \n",
       "33027    CASWGSPSSNTQYF  TVSWYQQALGQGPQFIFQYYREEENGRGNSPPRFSGLQFPNYSSEL...   \n",
       "\n",
       "       label  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  \n",
       "...      ...  \n",
       "33023      0  \n",
       "33024      0  \n",
       "33025      0  \n",
       "33026      0  \n",
       "33027      0  \n",
       "\n",
       "[33028 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = split[\"train\"]\n",
    "train_data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do these columns mean?\n",
    "\n",
    "The **epitope_aa** and the **tcr_full** columns are the protein (peptide) sequences for the epitope and the T-cell receptor, respectively. The letters correspond to the [standard amino acid codes](https://en.wikipedia.org/wiki/DNA_and_RNA_codon_tables).\n",
    "\n",
    "The **epitope_smi** column is the [SMILES](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system) notation for the chemical structure of the epitope. We won't use this information. Instead, the ESM-1b embedder should be sufficient for the input to our binary classification model.\n",
    "\n",
    "The **tcr** column are the first few amino acid letter codes for the T-cell receptor. It's just a label to distinguish similar TCR sequences.\n",
    "\n",
    "The **label** column is whether the two proteins bind. 0 = No. 1 = Yes.\n",
    "\n",
    "Our binary classification model should:\n",
    "* Use the **epitope_aa** and **tcr_full** embeddings as the input\n",
    "* Make a prediction if the **epitope_aa** and **tcr_full** will bind. This is the classification model's output. (0 = No, 1 = Yes)\n",
    "* Use the **label** as the ground truth of the binding (i.e. what the scientific experiment says)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data is not shuffled\n",
    "\n",
    "In the original datasets, the rows are sorted by label. We should randomize the row order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomize row order\n",
    "train_data = split[\"train\"].sample(frac = 1, random_state=816) \n",
    "validation_data = split[\"valid\"].sample(frac = 1, random_state=816)\n",
    "test_data = split[\"test\"].sample(frac = 1, random_state=816)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting embedding vectors for the protein sequences\n",
    "\n",
    "I like using [bio-embeddings](https://github.com/sacdallago/bio_embeddings) as my library for determining embedding vectors for proteins. In this case, we use Facebook's open-source [Evolutionary Scale Model (ESM-1b)](https://github.com/facebookresearch/esm). These embeddings turn the protein sequences into a vector of 1,280 numbers that the computer can use in a mathematical model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bio_embeddings.embed import ESM1bEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = ESM1bEmbedder()  # ESM/ESM1b (https://www.biorxiv.org/content/10.1101/622803v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of each embedding vector is: (1280,)\n",
      "The embedding vector for protein sequence DDCGKLFSGCDTNADCCEGYVCRLWCKLDW is: [ 0.08437371  0.24132712 -0.05913455 ...  0.05974105  0.07406684\n",
      "  0.1267224 ]\n"
     ]
    }
   ],
   "source": [
    "# Sequence from: https://www.uniprot.org/uniprot/P58426\n",
    "sequence = \"DDCGKLFSGCDTNADCCEGYVCRLWCKLDW\"\n",
    "per_residue_embedding = embedder.embed(sequence)\n",
    "per_protein_embedding = embedder.reduce_per_protein(per_residue_embedding)\n",
    "\n",
    "print(f\"The shape of each embedding vector is: {np.shape(per_protein_embedding)}\")\n",
    "print(f\"The embedding vector for protein sequence {sequence} is: {per_protein_embedding}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embedding function\n",
    "\n",
    "Here we create an embedding function (calling bio-embeddings) that we can use in the `pandas.apply` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(sequence: str, embedder) -> np.array:\n",
    "    \"\"\"Gets the sequence embedding for the protein sequence\n",
    "\n",
    "    Args:\n",
    "        sequence(str): The protein sequence as a string (e.g. \"DDTNAWCLCDR\")\n",
    "        embedder:  The bio-embedder object\n",
    "\n",
    "    Returns:\n",
    "        The per protein sequence embedding vector (1280,1)\n",
    "    \"\"\"\n",
    "    per_residue_embedding = embedder.embed(sequence)\n",
    "    per_protein_embedding = embedder.reduce_per_protein(per_residue_embedding)\n",
    "\n",
    "    return np.array(per_protein_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_both_embedding(\n",
    "    sequence_A: str, sequence_B: str, embedder\n",
    ") -> tuple[np.array, np.array]:\n",
    "    \"\"\"Gets the sequence embedding for both protein sequences\n",
    "\n",
    "    This allows us to process the embeddings in one pass.\n",
    "\n",
    "    Args:\n",
    "        sequence_A(str): The protein sequence as string A (e.g. \"DDTNAWCLCDR\")\n",
    "        sequence_B(str): The protein sequence as string B (e.g. \"DDSEQVENCES\")\n",
    "        embedder:  The bio-embedder object\n",
    "\n",
    "    Returns:\n",
    "        The per protein sequence embedding vectors for A and B (1280,1), (1280,1)\n",
    "    \"\"\"\n",
    "\n",
    "    return get_embedding(sequence_A, embedder), get_embedding(sequence_B, embedder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get embedding for the protein sequences\n",
    "\n",
    "Returns the embedding vectors for the **epitope** and **tcr**. We'll use these embeddings as input to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1c359848d64b33974118ab5c240654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Embedding protein sequences:   0%|          | 0/33028 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_data[\u001b[39m\"\u001b[39m\u001b[39mepitope_vector\u001b[39m\u001b[39m\"\u001b[39m], train_data[\u001b[39m\"\u001b[39m\u001b[39mtcr_vector\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m train_data\u001b[39m.\u001b[39;49mprogress_apply(\n\u001b[1;32m      2\u001b[0m     \u001b[39mlambda\u001b[39;49;00m x: get_both_embedding(x[\u001b[39m\"\u001b[39;49m\u001b[39mepitope_aa\u001b[39;49m\u001b[39m\"\u001b[39;49m], x[\u001b[39m\"\u001b[39;49m\u001b[39mtcr_full\u001b[39;49m\u001b[39m\"\u001b[39;49m], embedder), axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m\n\u001b[1;32m      3\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tdc-tcr-epitope-binding-affinity-env/lib/python3.9/site-packages/tqdm/std.py:814\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[39m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[1;32m    812\u001b[0m \u001b[39m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 814\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(df, df_function)(wrapper, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    815\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    816\u001b[0m     t\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/tdc-tcr-epitope-binding-affinity-env/lib/python3.9/site-packages/pandas/core/frame.py:9565\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9554\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[1;32m   9556\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[1;32m   9557\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   9558\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9563\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[1;32m   9564\u001b[0m )\n\u001b[0;32m-> 9565\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tdc-tcr-epitope-binding-affinity-env/lib/python3.9/site-packages/pandas/core/apply.py:746\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[1;32m    744\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[0;32m--> 746\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/anaconda3/envs/tdc-tcr-epitope-binding-affinity-env/lib/python3.9/site-packages/pandas/core/apply.py:873\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 873\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[1;32m    875\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[1;32m    876\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/anaconda3/envs/tdc-tcr-epitope-binding-affinity-env/lib/python3.9/site-packages/pandas/core/apply.py:889\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    887\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[1;32m    888\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[1;32m    890\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    891\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    892\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    893\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tdc-tcr-epitope-binding-affinity-env/lib/python3.9/site-packages/tqdm/std.py:809\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    804\u001b[0m     \u001b[39m# update tbar correctly\u001b[39;00m\n\u001b[1;32m    805\u001b[0m     \u001b[39m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[1;32m    806\u001b[0m     \u001b[39m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[1;32m    807\u001b[0m     \u001b[39m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[1;32m    808\u001b[0m     t\u001b[39m.\u001b[39mupdate(n\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m t\u001b[39m.\u001b[39mtotal \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mn \u001b[39m<\u001b[39m t\u001b[39m.\u001b[39mtotal \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m)\n\u001b[0;32m--> 809\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m train_data[\u001b[39m\"\u001b[39m\u001b[39mepitope_vector\u001b[39m\u001b[39m\"\u001b[39m], train_data[\u001b[39m\"\u001b[39m\u001b[39mtcr_vector\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m train_data\u001b[39m.\u001b[39mprogress_apply(\n\u001b[0;32m----> 2\u001b[0m     \u001b[39mlambda\u001b[39;00m x: get_both_embedding(x[\u001b[39m\"\u001b[39;49m\u001b[39mepitope_aa\u001b[39;49m\u001b[39m\"\u001b[39;49m], x[\u001b[39m\"\u001b[39;49m\u001b[39mtcr_full\u001b[39;49m\u001b[39m\"\u001b[39;49m], embedder), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m      3\u001b[0m )\n",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m, in \u001b[0;36mget_both_embedding\u001b[0;34m(sequence_A, sequence_B, embedder)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_both_embedding\u001b[39m(\n\u001b[1;32m      2\u001b[0m     sequence_A: \u001b[39mstr\u001b[39m, sequence_B: \u001b[39mstr\u001b[39m, embedder\n\u001b[1;32m      3\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m (np\u001b[39m.\u001b[39marray, np\u001b[39m.\u001b[39marray):\n\u001b[1;32m      4\u001b[0m     \u001b[39m\"\"\"Gets the sequence embedding for both protein sequences\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[39m    This allows us to process the embeddings in one pass.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m        The per protein sequence embedding vectors for A and B (1280,1), (1280,1)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     \u001b[39mreturn\u001b[39;00m get_embedding(sequence_A, embedder), get_embedding(sequence_B, embedder)\n",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m, in \u001b[0;36mget_embedding\u001b[0;34m(sequence, embedder)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_embedding\u001b[39m(sequence: \u001b[39mstr\u001b[39m, embedder) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39marray:\n\u001b[1;32m      2\u001b[0m     \u001b[39m\"\"\"Gets the sequence embedding for the protein sequence\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m        The per protein sequence embedding vector (1280,1)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     per_residue_embedding \u001b[39m=\u001b[39m embedder\u001b[39m.\u001b[39;49membed(sequence)\n\u001b[1;32m     12\u001b[0m     per_protein_embedding \u001b[39m=\u001b[39m embedder\u001b[39m.\u001b[39mreduce_per_protein(per_residue_embedding)\n\u001b[1;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(per_protein_embedding)\n",
      "File \u001b[0;32m~/anaconda3/envs/tdc-tcr-epitope-binding-affinity-env/lib/python3.9/site-packages/bio_embeddings/embed/esm_embedder.py:45\u001b[0m, in \u001b[0;36mESMEmbedderBase.embed\u001b[0;34m(self, sequence)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39membed\u001b[39m(\u001b[39mself\u001b[39m, sequence: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ndarray:\n\u001b[0;32m---> 45\u001b[0m     [embedding] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_batch([sequence])\n\u001b[1;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m embedding\n",
      "File \u001b[0;32m~/anaconda3/envs/tdc-tcr-epitope-binding-affinity-env/lib/python3.9/site-packages/bio_embeddings/embed/esm_embedder.py:56\u001b[0m, in \u001b[0;36mESMEmbedderBase.embed_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m batch_labels, batch_strs, batch_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_converter(data)\n\u001b[1;32m     55\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 56\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model(\n\u001b[1;32m     57\u001b[0m         batch_tokens\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_device), repr_layers\u001b[39m=\u001b[39;49m[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_picked_layer]\n\u001b[1;32m     58\u001b[0m     )\n\u001b[1;32m     59\u001b[0m token_embeddings \u001b[39m=\u001b[39m results[\u001b[39m\"\u001b[39m\u001b[39mrepresentations\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_picked_layer]\n\u001b[1;32m     61\u001b[0m \u001b[39m# Generate per-sequence embeddings via averaging\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39m# NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tdc-tcr-epitope-binding-affinity-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/tdc-tcr-epitope-binding-affinity-env/lib/python3.9/site-packages/esm/model.py:155\u001b[0m, in \u001b[0;36mProteinBertModel.forward\u001b[0;34m(self, tokens, repr_layers, need_head_weights, return_contacts)\u001b[0m\n\u001b[1;32m    152\u001b[0m     padding_mask \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfor\u001b[39;00m layer_idx, layer \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers):\n\u001b[0;32m--> 155\u001b[0m     x, attn \u001b[39m=\u001b[39m layer(x, self_attn_padding_mask\u001b[39m=\u001b[39;49mpadding_mask, need_head_weights\u001b[39m=\u001b[39;49mneed_head_weights)\n\u001b[1;32m    156\u001b[0m     \u001b[39mif\u001b[39;00m (layer_idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39min\u001b[39;00m repr_layers:\n\u001b[1;32m    157\u001b[0m         hidden_representations[layer_idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tdc-tcr-epitope-binding-affinity-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/tdc-tcr-epitope-binding-affinity-env/lib/python3.9/site-packages/esm/modules.py:107\u001b[0m, in \u001b[0;36mTransformerLayer.forward\u001b[0;34m(self, x, self_attn_mask, self_attn_padding_mask, need_head_weights)\u001b[0m\n\u001b[1;32m    105\u001b[0m residual \u001b[39m=\u001b[39m x\n\u001b[1;32m    106\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn_layer_norm(x)\n\u001b[0;32m--> 107\u001b[0m x, attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    108\u001b[0m     query\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    109\u001b[0m     key\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    110\u001b[0m     value\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    111\u001b[0m     key_padding_mask\u001b[39m=\u001b[39;49mself_attn_padding_mask,\n\u001b[1;32m    112\u001b[0m     need_weights\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    113\u001b[0m     need_head_weights\u001b[39m=\u001b[39;49mneed_head_weights,\n\u001b[1;32m    114\u001b[0m     attn_mask\u001b[39m=\u001b[39;49mself_attn_mask,\n\u001b[1;32m    115\u001b[0m )\n\u001b[1;32m    116\u001b[0m x \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m x\n\u001b[1;32m    118\u001b[0m residual \u001b[39m=\u001b[39m x\n",
      "File \u001b[0;32m~/anaconda3/envs/tdc-tcr-epitope-binding-affinity-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/tdc-tcr-epitope-binding-affinity-env/lib/python3.9/site-packages/esm/multihead_attention.py:198\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, incremental_state, need_weights, static_kv, attn_mask, before_softmax, need_head_weights)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    188\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menable_torch_version\n\u001b[1;32m    189\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39monnx_trace\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m need_head_weights\n\u001b[1;32m    196\u001b[0m ):\n\u001b[1;32m    197\u001b[0m     \u001b[39massert\u001b[39;00m key \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m    199\u001b[0m         query,\n\u001b[1;32m    200\u001b[0m         key,\n\u001b[1;32m    201\u001b[0m         value,\n\u001b[1;32m    202\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim,\n\u001b[1;32m    203\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m    204\u001b[0m         torch\u001b[39m.\u001b[39;49mempty([\u001b[39m0\u001b[39;49m]),\n\u001b[1;32m    205\u001b[0m         torch\u001b[39m.\u001b[39;49mcat((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_proj\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mk_proj\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mv_proj\u001b[39m.\u001b[39;49mbias)),\n\u001b[1;32m    206\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k,\n\u001b[1;32m    207\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v,\n\u001b[1;32m    208\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m    209\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout,\n\u001b[1;32m    210\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    211\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    212\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m    213\u001b[0m         key_padding_mask,\n\u001b[1;32m    214\u001b[0m         need_weights,\n\u001b[1;32m    215\u001b[0m         attn_mask,\n\u001b[1;32m    216\u001b[0m         use_separate_proj_weight\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    217\u001b[0m         q_proj_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_proj\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    218\u001b[0m         k_proj_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mk_proj\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    219\u001b[0m         v_proj_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mv_proj\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    220\u001b[0m     )\n\u001b[1;32m    222\u001b[0m \u001b[39mif\u001b[39;00m incremental_state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     saved_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_input_buffer(incremental_state)\n",
      "File \u001b[0;32m~/anaconda3/envs/tdc-tcr-epitope-binding-affinity-env/lib/python3.9/site-packages/torch/nn/functional.py:4978\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   4976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4977\u001b[0m         b_q, b_k, b_v \u001b[39m=\u001b[39m in_proj_bias\u001b[39m.\u001b[39mchunk(\u001b[39m3\u001b[39m)\n\u001b[0;32m-> 4978\u001b[0m     q, k, v \u001b[39m=\u001b[39m _in_projection(query, key, value, q_proj_weight, k_proj_weight, v_proj_weight, b_q, b_k, b_v)\n\u001b[1;32m   4980\u001b[0m \u001b[39m# prep attention mask\u001b[39;00m\n\u001b[1;32m   4981\u001b[0m \u001b[39mif\u001b[39;00m attn_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tdc-tcr-epitope-binding-affinity-env/lib/python3.9/site-packages/torch/nn/functional.py:4787\u001b[0m, in \u001b[0;36m_in_projection\u001b[0;34m(q, k, v, w_q, w_k, w_v, b_q, b_k, b_v)\u001b[0m\n\u001b[1;32m   4785\u001b[0m \u001b[39massert\u001b[39;00m b_k \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m b_k\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (Eq,), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpecting key bias shape of \u001b[39m\u001b[39m{\u001b[39;00m(Eq,)\u001b[39m}\u001b[39;00m\u001b[39m, but got \u001b[39m\u001b[39m{\u001b[39;00mb_k\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4786\u001b[0m \u001b[39massert\u001b[39;00m b_v \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m b_v\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (Eq,), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpecting value bias shape of \u001b[39m\u001b[39m{\u001b[39;00m(Eq,)\u001b[39m}\u001b[39;00m\u001b[39m, but got \u001b[39m\u001b[39m{\u001b[39;00mb_v\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 4787\u001b[0m \u001b[39mreturn\u001b[39;00m linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)\n",
      "File \u001b[0;32m~/anaconda3/envs/tdc-tcr-epitope-binding-affinity-env/lib/python3.9/site-packages/torch/nn/functional.py:1847\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight):\n\u001b[1;32m   1846\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n\u001b[0;32m-> 1847\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data[\"epitope_vector\"], train_data[\"tcr_vector\"] = train_data.progress_apply(\n",
    "    lambda x: get_both_embedding(x[\"epitope_aa\"], x[\"tcr_full\"], embedder), axis=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdc-tcr-epitope-binding-affinity-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6efcf45f9bec60f919453d54d1718a4d1526f09dbabed9268bf472b0ad8085e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
