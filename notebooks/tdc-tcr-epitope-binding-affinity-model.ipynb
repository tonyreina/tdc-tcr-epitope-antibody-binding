{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bioinformatics model for protein therapeutics\n",
    "\n",
    "We'll use the [Therapeutics Data Commons](https://tdcommons.ai/) Python package to download open-source ([CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)) datasets that are meaningful in pharmaceutical research. In this notebook, we'll use a dataset called [TCR-Epitope Binding Affinity](https://tdcommons.ai/multi_pred_tasks/tcrepitope/).\n",
    "\n",
    "![TCR-epitope binding](tcr-epitope-binding.png)\n",
    "\n",
    "We show how to create a deep learning model for predicting if a T-cell receptor (TCR) and protein epitope will bind to each other. A model that can predict how well a TCR bindings to an epitope can lead to more effective treatments that use immunotherapy. For example, in anti-cancer therapies it is important for the T-cell receptor to bind to the protein marker in the cancer cell so that the T-cell (actually the T-cell's friends in the immune system) can kill the cancer cell.\n",
    "\n",
    "We'll see how to use the open-sourced [bio-embeddings](https://docs.bioembeddings.com/v0.2.3/) Python library to get the latest state-of-the-art AI model for embedding the protein sequences. In this case, we use Facebook's open-source [Evolutionary Scale Model (ESM-1b)](https://github.com/facebookresearch/esm). These embeddings turn the protein sequences into a vector of 1,280 numbers that the computer can use in a mathematical model. The vector of numbers uniquely encodes (aka embeds) a protein sequence in the same way that the [Dewey Decimal System](https://en.wikipedia.org/wiki/Dewey_Decimal_Classification) uniquely encodes a book into a set of numbers (and letters). This embedding space is also referred to as a [latent space](https://en.wikipedia.org/wiki/Latent_space#:~:text=A%20latent%20space%2C%20also%20known,another%20in%20the%20latent%20space).\n",
    "\n",
    "Then, we'll show how to combine this embedding with a simple neural network to create a [binary classifier](https://en.wikipedia.org/wiki/Binary_classification) for the TCR-epitope binding affinity prediction (True=They Bind, False=They don't bind).\n",
    "\n",
    "![encoder-decoder Dewey Decimal](encoder-decoder.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas(\n",
    "    desc=\"Embedding protein sequences\"\n",
    ")  # Create fancy progress bar for Pandas apply"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the dataset\n",
    "\n",
    "We are using the TDC dataset for [TCR-Epitope Binding Affinity Prediction Task](https://tdcommons.ai/multi_pred_tasks/tcrepitope/).\n",
    "\n",
    "From the TDC website:\n",
    "\n",
    ">T-cells are an integral part of the adaptive immune system, whose survival, proliferation, activation and function are all governed by the interaction of their T-cell receptor (TCR) with immunogenic peptides (epitopes). A large repertoire of T-cell receptors with different specificity is needed to provide protection against a wide range of pathogens. This new task aims to predict the binding affinity given a pair of TCR sequence and epitope sequence.\n",
    "\n",
    ">Weber et al.\n",
    "Dataset Description: The dataset is from Weber et al. who assemble a large and diverse data from the VDJ database and ImmuneCODE project. It uses human TCR-beta chain sequences. Since this dataset is highly imbalanced, the authors exclude epitopes with less than 15 associated TCR sequences and downsample to a limit of 400 TCRs per epitope. The dataset contains amino acid sequences either for the entire TCR or only for the hypervariable CDR3 loop. Epitopes are available as amino acid sequences. Since Weber et al. proposed to represent the peptides as SMILES strings (which reformulates the problem to protein-ligand binding prediction) the SMILES strings of the epitopes are also included. 50% negative samples were generated by shuffling the pairs, i.e. associating TCR sequences with epitopes they have not been shown to bind.\n",
    "\n",
    ">Task Description: Binary classification. Given the epitope (a peptide, either represented as amino acid sequence or as SMILES) and a T-cell receptor (amino acid sequence, either of the full protein complex or only of the hypervariable CDR3 loop), predict whether the epitope binds to the TCR.\n",
    "\n",
    ">Dataset Statistics: 47,182 TCR-Epitope pairs between 192 epitopes and 23,139 TCRs.\n",
    "\n",
    ">References:\n",
    "\n",
    "1. Weber, Anna, Jannis Born, and María Rodriguez Martínez. “TITAN: T-cell receptor specificity prediction with bimodal attention networks.” Bioinformatics 37.Supplement_1 (2021): i237-i244.\n",
    "\n",
    "2. Bagaev, Dmitry V., et al. “VDJdb in 2019: database extension, new analysis infrastructure and a T-cell receptor motif compendium.” Nucleic Acids Research 48.D1 (2020): D1057-D1062.\n",
    "\n",
    "3. Dines, Jennifer N., et al. “The immunerace study: A prospective multicohort study of immune response action to covid-19 events with the immunecode™ open access database.” medRxiv (2020).\n",
    "\n",
    ">Dataset License: CC BY 4.0.\n",
    "\n",
    ">Contributed by: Anna Weber and Jannis Born.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the TCR-epitope dataset\n",
    "\n",
    "Download and split randomly into 70% training data, 10% validation data, and 20% testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdc.multi_pred import TCREpitopeBinding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "data = TCREpitopeBinding(name=\"weber\", path=\"./data\")  # Download the dataset\n",
    "split = data.get_split(\n",
    "    method=\"random\", seed=816, frac=[0.7, 0.1, 0.2]\n",
    ")  # Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: \t\t33,028 proteins\n",
      "Validation dataset size: \t 4,718 proteins\n",
      "Test dataset size: \t\t 9,436 proteins\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train dataset size: \\t\\t{len(split['train']):6,d} proteins\")\n",
    "print(f\"Validation dataset size: \\t{len(split['valid']):6,d} proteins\")\n",
    "print(f\"Test dataset size: \\t\\t{len(split['test']):6,d} proteins\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epitope_aa</th>\n",
       "      <th>epitope_smi</th>\n",
       "      <th>tcr</th>\n",
       "      <th>tcr_full</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FLKEKGGL</td>\n",
       "      <td>CC(C)C[C@H](NC(=O)CNC(=O)CNC(=O)[C@H](CCCCN)NC...</td>\n",
       "      <td>CSVWGTGKTYEQYF</td>\n",
       "      <td>SAVISQKPSRDICQRGTSLTIQCQVDSQVTMMFWYRQQPGQSLTLI...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FLKEKGGL</td>\n",
       "      <td>CC(C)C[C@H](NC(=O)CNC(=O)CNC(=O)[C@H](CCCCN)NC...</td>\n",
       "      <td>CSVWGEGRSYEQYF</td>\n",
       "      <td>SAVISQKPSRDICQRGTSLTIQCQVDSQVTMMFWYRQQPGQSLTLI...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FLKEKGGL</td>\n",
       "      <td>CC(C)C[C@H](NC(=O)CNC(=O)CNC(=O)[C@H](CCCCN)NC...</td>\n",
       "      <td>CSATILAGVPYGEQYF</td>\n",
       "      <td>GAVVSQHPSWVICKSGTSVKIECRSLDFQATTMFWYRQFPKQSLML...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FLKEKGGL</td>\n",
       "      <td>CC(C)C[C@H](NC(=O)CNC(=O)CNC(=O)[C@H](CCCCN)NC...</td>\n",
       "      <td>CASSFDREVTGELFF</td>\n",
       "      <td>GAGVSQTPSNKVTEKGKYVELRCDPISGHTALYWYRQSLGQGPEFL...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FLKEKGGL</td>\n",
       "      <td>CC(C)C[C@H](NC(=O)CNC(=O)CNC(=O)[C@H](CCCCN)NC...</td>\n",
       "      <td>CASSVGAGTEAFF</td>\n",
       "      <td>DGGITQSPKYLFRKEGQNVTLSCEQNLNHDAMYWYRQDPGQGLRLI...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33023</th>\n",
       "      <td>KLMNIQQKL</td>\n",
       "      <td>CC[C@H](C)[C@H](NC(=O)[C@H](CC(N)=O)NC(=O)[C@H...</td>\n",
       "      <td>CASSKPGLTDTQYF</td>\n",
       "      <td>NAGVTQTPKFQVLKTGQSMTLQCAQDMNHEYMSWYRQDPGMGLRLI...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33024</th>\n",
       "      <td>TLIGDCATV</td>\n",
       "      <td>CC[C@H](C)[C@H](NC(=O)[C@H](CC(C)C)NC(=O)[C@@H...</td>\n",
       "      <td>CASSPGQGRTHYGYTF</td>\n",
       "      <td>NAGVTQTPKFRILKIGQSMTLQCAQDMNHNYMYWYRQDPGMGLKLI...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33025</th>\n",
       "      <td>LLFGYPVYV</td>\n",
       "      <td>CC(C)C[C@H](N)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H]...</td>\n",
       "      <td>CASSGGSLNTEAFF</td>\n",
       "      <td>NAGVTQTPKFQVLKTGQSMTLQCAQDMNHEYMSWYRQDPGMGLRLI...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33026</th>\n",
       "      <td>ISPRTLNAW</td>\n",
       "      <td>CC[C@H](C)[C@H](N)C(=O)N[C@@H](CO)C(=O)N1CCC[C...</td>\n",
       "      <td>CASSPSAAMNTEAFF</td>\n",
       "      <td>TVSWYQQALGQGPQFIFQYYREEENGRGNSPPRFSGLQFPNYSSEL...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33027</th>\n",
       "      <td>ILHCANFNV</td>\n",
       "      <td>CC[C@H](C)[C@H](N)C(=O)N[C@@H](CC(C)C)C(=O)N[C...</td>\n",
       "      <td>CASWGSPSSNTQYF</td>\n",
       "      <td>TVSWYQQALGQGPQFIFQYYREEENGRGNSPPRFSGLQFPNYSSEL...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33028 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      epitope_aa                                        epitope_smi  \\\n",
       "0       FLKEKGGL  CC(C)C[C@H](NC(=O)CNC(=O)CNC(=O)[C@H](CCCCN)NC...   \n",
       "1       FLKEKGGL  CC(C)C[C@H](NC(=O)CNC(=O)CNC(=O)[C@H](CCCCN)NC...   \n",
       "2       FLKEKGGL  CC(C)C[C@H](NC(=O)CNC(=O)CNC(=O)[C@H](CCCCN)NC...   \n",
       "3       FLKEKGGL  CC(C)C[C@H](NC(=O)CNC(=O)CNC(=O)[C@H](CCCCN)NC...   \n",
       "4       FLKEKGGL  CC(C)C[C@H](NC(=O)CNC(=O)CNC(=O)[C@H](CCCCN)NC...   \n",
       "...          ...                                                ...   \n",
       "33023  KLMNIQQKL  CC[C@H](C)[C@H](NC(=O)[C@H](CC(N)=O)NC(=O)[C@H...   \n",
       "33024  TLIGDCATV  CC[C@H](C)[C@H](NC(=O)[C@H](CC(C)C)NC(=O)[C@@H...   \n",
       "33025  LLFGYPVYV  CC(C)C[C@H](N)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H]...   \n",
       "33026  ISPRTLNAW  CC[C@H](C)[C@H](N)C(=O)N[C@@H](CO)C(=O)N1CCC[C...   \n",
       "33027  ILHCANFNV  CC[C@H](C)[C@H](N)C(=O)N[C@@H](CC(C)C)C(=O)N[C...   \n",
       "\n",
       "                    tcr                                           tcr_full  \\\n",
       "0        CSVWGTGKTYEQYF  SAVISQKPSRDICQRGTSLTIQCQVDSQVTMMFWYRQQPGQSLTLI...   \n",
       "1        CSVWGEGRSYEQYF  SAVISQKPSRDICQRGTSLTIQCQVDSQVTMMFWYRQQPGQSLTLI...   \n",
       "2      CSATILAGVPYGEQYF  GAVVSQHPSWVICKSGTSVKIECRSLDFQATTMFWYRQFPKQSLML...   \n",
       "3       CASSFDREVTGELFF  GAGVSQTPSNKVTEKGKYVELRCDPISGHTALYWYRQSLGQGPEFL...   \n",
       "4         CASSVGAGTEAFF  DGGITQSPKYLFRKEGQNVTLSCEQNLNHDAMYWYRQDPGQGLRLI...   \n",
       "...                 ...                                                ...   \n",
       "33023    CASSKPGLTDTQYF  NAGVTQTPKFQVLKTGQSMTLQCAQDMNHEYMSWYRQDPGMGLRLI...   \n",
       "33024  CASSPGQGRTHYGYTF  NAGVTQTPKFRILKIGQSMTLQCAQDMNHNYMYWYRQDPGMGLKLI...   \n",
       "33025    CASSGGSLNTEAFF  NAGVTQTPKFQVLKTGQSMTLQCAQDMNHEYMSWYRQDPGMGLRLI...   \n",
       "33026   CASSPSAAMNTEAFF  TVSWYQQALGQGPQFIFQYYREEENGRGNSPPRFSGLQFPNYSSEL...   \n",
       "33027    CASWGSPSSNTQYF  TVSWYQQALGQGPQFIFQYYREEENGRGNSPPRFSGLQFPNYSSEL...   \n",
       "\n",
       "       label  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  \n",
       "...      ...  \n",
       "33023      0  \n",
       "33024      0  \n",
       "33025      0  \n",
       "33026      0  \n",
       "33027      0  \n",
       "\n",
       "[33028 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = split[\"train\"]\n",
    "train_data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do these columns mean?\n",
    "\n",
    "The **epitope_aa** and the **tcr_full** columns are the protein (peptide) sequences for the epitope and the T-cell receptor, respectively. The letters correspond to the [standard amino acid codes](https://en.wikipedia.org/wiki/DNA_and_RNA_codon_tables).\n",
    "\n",
    "The **epitope_smi** column is the [SMILES](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system) notation for the chemical structure of the epitope. We won't use this information. Instead, the ESM-1b embedder should be sufficient for the input to our binary classification model.\n",
    "\n",
    "The **tcr** column are the first few amino acid letter codes for the T-cell receptor. It's just a label to distinguish similar TCR sequences.\n",
    "\n",
    "The **label** column is whether the two proteins bind. 0 = No. 1 = Yes.\n",
    "\n",
    "Our binary classification model should:\n",
    "* Use the **epitope_aa** and **tcr_full** embeddings as the input\n",
    "* Make a prediction if the **epitope_aa** and **tcr_full** will bind. This is the classification model's output. (0 = No, 1 = Yes)\n",
    "* Use the **label** as the ground truth of the binding (i.e. what the scientific experiment says)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data is not shuffled\n",
    "\n",
    "In the original datasets, the rows are sorted by label. We should randomize the row order when training our classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomize row order\n",
    "train_data = split[\"train\"].sample(frac = 1, random_state=816) \n",
    "validation_data = split[\"valid\"].sample(frac = 1, random_state=816)\n",
    "test_data = split[\"test\"].sample(frac = 1, random_state=816)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting embedding vectors for the protein sequences\n",
    "\n",
    "I like using [bio-embeddings](https://github.com/sacdallago/bio_embeddings) as my library for determining embedding vectors for proteins. In this case, we use Facebook's open-source [Evolutionary Scale Model (ESM-1b)](https://github.com/facebookresearch/esm). These embeddings turn the protein sequences into a vector of 1,280 numbers that the computer can use in a mathematical model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bio_embeddings.embed import ESM1bEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = ESM1bEmbedder()  # ESM/ESM1b (https://www.biorxiv.org/content/10.1101/622803v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of each embedding vector is: (1280,)\n",
      "The embedding vector for protein sequence DDCGKLFSGCDTNADCCEGYVCRLWCKLDW is: [ 0.08437356  0.24132735 -0.05913459 ...  0.05974118  0.07406691\n",
      "  0.12672262]\n"
     ]
    }
   ],
   "source": [
    "# Sequence from: https://www.uniprot.org/uniprot/P58426\n",
    "sequence = \"DDCGKLFSGCDTNADCCEGYVCRLWCKLDW\"\n",
    "per_residue_embedding = embedder.embed(sequence)\n",
    "per_protein_embedding = embedder.reduce_per_protein(per_residue_embedding)\n",
    "\n",
    "print(f\"The shape of each embedding vector is: {np.shape(per_protein_embedding)}\")\n",
    "print(f\"The embedding vector for protein sequence {sequence} is: {per_protein_embedding}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embedding function\n",
    "\n",
    "Here we create an embedding function (calling bio-embeddings) that we can use in the `pandas.apply` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(sequence: str, embedder) -> np.array:\n",
    "    \"\"\"Gets the sequence embedding for the protein sequence\n",
    "\n",
    "    Args:\n",
    "        sequence(str): The protein sequence as a string (e.g. \"DDTNAWCLCDR\")\n",
    "        embedder:  The bio-embedder object\n",
    "\n",
    "    Returns:\n",
    "        The per protein sequence embedding vector (1280,1)\n",
    "    \"\"\"\n",
    "    per_residue_embedding = embedder.embed(sequence)\n",
    "    per_protein_embedding = embedder.reduce_per_protein(per_residue_embedding)\n",
    "\n",
    "    return np.array(per_protein_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_both_embedding(\n",
    "    sequence_A: str, sequence_B: str, embedder\n",
    ") -> tuple[np.array, np.array]:\n",
    "    \"\"\"Gets the sequence embedding for both protein sequences\n",
    "\n",
    "    This allows us to process the embeddings in one pass.\n",
    "\n",
    "    Args:\n",
    "        sequence_A(str): The protein sequence as string A (e.g. \"DDTNAWCLCDR\")\n",
    "        sequence_B(str): The protein sequence as string B (e.g. \"DDSEQVENCES\")\n",
    "        embedder:  The bio-embedder object\n",
    "\n",
    "    Returns:\n",
    "        The per protein sequence embedding vectors for A and B (1280,1), (1280,1)\n",
    "    \"\"\"\n",
    "\n",
    "    return get_embedding(sequence_A, embedder), get_embedding(sequence_B, embedder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get embedding for the protein sequences\n",
    "\n",
    "Returns the embedding vectors for the **epitope** and **tcr**. We'll use these embeddings as input to our model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of CPU/GPU Hardware \n",
    "\n",
    "On a CPU with 8 virtual cores (4 physical cores), the `get_both_embedding` will take ~6 hours to complete. The ESM-1b model is large so you'll need ~ 16 GB of RAM.\n",
    "\n",
    "I chose an [AWS P3 2xlarge](https://aws.amazon.com/ec2/instance-types/p3/) which includes a NVidia V100 GPU to speed that up to about 30-40 minutes. On a GPU, ESM-1b only uses about 4 GB of GPU RAM.\n",
    "\n",
    "### Caching the embeddings to a file\n",
    "\n",
    "In order to spare us from having to re-run a 30 minute embedding, the code below first checks if a previous run of emebddings has been cached to a pickle file. If so, then we just load that file rather than re-run the `get_both_embedding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab85792926a41128bc84e27b28a2614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Embedding protein sequences:   0%|          | 0/33028 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not os.path.exists(\"train_data.pkl\"):\n",
    "    train_data[\"epitope_vector\"], train_data[\"tcr_vector\"] = train_data.progress_apply(\n",
    "        lambda x: get_both_embedding(x[\"epitope_aa\"], x[\"tcr_full\"], embedder), axis=\"columns\", result_type=\"expand\"\n",
    "    )\n",
    "\n",
    "    train_data.to_pickle(\"train_data.pkl\")\n",
    "else:\n",
    "    train_data = pd.read_pickle(\"train_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"validation_data.pkl\"):\n",
    "    validation_data[\"epitope_vector\"], validation_data[\"tcr_vector\"] = validation_data.progress_apply(\n",
    "        lambda x: get_both_embedding(x[\"epitope_aa\"], x[\"tcr_full\"], embedder), axis=\"columns\", result_type=\"expand\"\n",
    "    )\n",
    "    validation_data.to_pickle(\"validation_data.pkl\")\n",
    "else:\n",
    "    validation_data = pd.read_pickle(\"validation_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"test_data.pkl\"):\n",
    "    test_data[\"epitope_vector\"], test_data[\"tcr_vector\"] = test_data.progress_apply(\n",
    "        lambda x: get_both_embedding(x[\"epitope_aa\"], x[\"tcr_full\"], embedder), axis=\"columns\", result_type=\"expand\"\n",
    "    )\n",
    "    test_data.to_pickle(\"test_data.pkl\")\n",
    "else:\n",
    "    test_data = pd.read_pickle(\"test_data.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdc-tcr-epitope-binding-affinity-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:58:50) \n[GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6efcf45f9bec60f919453d54d1718a4d1526f09dbabed9268bf472b0ad8085e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
